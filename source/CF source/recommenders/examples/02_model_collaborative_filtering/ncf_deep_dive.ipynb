{"cells":[{"cell_type":"markdown","metadata":{"id":"Gn3kscT7nPxw"},"source":["<i>Copyright (c) Recommenders contributors.</i>\n","\n","<i>Licensed under the MIT License.</i>"]},{"cell_type":"markdown","metadata":{"id":"udM2iYeDnPx0"},"source":["# Neural Collaborative Filtering (NCF)\n","\n","This notebook serves as an introduction to Neural Collaborative Filtering (NCF), which is an innovative algorithm based on deep neural networks to tackle the key problem in recommendation — collaborative filtering — on the basis of implicit feedback."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XdJmue9ur3Vi","executionInfo":{"status":"ok","timestamp":1702378866946,"user_tz":-420,"elapsed":29987,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}},"outputId":"952091c0-e497-4f41-f2f3-4a478ccb13b8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab_me/DS300/recommenders/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ihKhlnxWr4uW","executionInfo":{"status":"ok","timestamp":1702378866947,"user_tz":-420,"elapsed":7,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}},"outputId":"496603c4-eb00-4b0f-a632-0512ceef4587"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_me/DS300/recommenders\n"]}]},{"cell_type":"markdown","metadata":{"id":"TiwkKc5inPx1"},"source":["## 0 Global Settings and Imports"]},{"cell_type":"code","source":["!pip install scrapbook\n","!pip install papermill\n","!pip install cornac\n","!pip install retrying\n","!pip install pandera"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sWnsOYxTsLxe","executionInfo":{"status":"ok","timestamp":1702378972351,"user_tz":-420,"elapsed":57163,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}},"outputId":"61e07d81-e7fd-49c5-fbbc-4e4b7e9060bf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scrapbook\n","  Downloading scrapbook-0.5.0-py3-none-any.whl (34 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from scrapbook) (1.5.3)\n","Collecting papermill (from scrapbook)\n","  Downloading papermill-2.5.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from scrapbook) (4.19.2)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from scrapbook) (7.34.0)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from scrapbook) (10.0.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (67.7.2)\n","Collecting jedi>=0.16 (from ipython->scrapbook)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (3.0.41)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->scrapbook) (4.9.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->scrapbook) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->scrapbook) (2023.11.2)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->scrapbook) (0.31.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->scrapbook) (0.13.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scrapbook) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scrapbook) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->scrapbook) (1.23.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (8.1.7)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (6.0.1)\n","Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (5.9.2)\n","Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (0.9.0)\n","Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (2.31.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (0.4)\n","Requirement already satisfied: tenacity>=5.0.2 in /usr/local/lib/python3.10/dist-packages (from papermill->scrapbook) (8.2.3)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->scrapbook) (0.8.3)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (6.1.12)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (5.5.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.2->papermill->scrapbook) (2.19.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->scrapbook) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->scrapbook) (0.2.12)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->scrapbook) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->papermill->scrapbook) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->papermill->scrapbook) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->papermill->scrapbook) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->papermill->scrapbook) (2023.11.17)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill->scrapbook) (23.2.1)\n","Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill->scrapbook) (6.3.2)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbclient>=0.2.0->papermill->scrapbook) (4.1.0)\n","Installing collected packages: jedi, papermill, scrapbook\n","Successfully installed jedi-0.19.1 papermill-2.5.0 scrapbook-0.5.0\n","Requirement already satisfied: papermill in /usr/local/lib/python3.10/dist-packages (2.5.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from papermill) (8.1.7)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from papermill) (6.0.1)\n","Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.10/dist-packages (from papermill) (5.9.2)\n","Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from papermill) (0.9.0)\n","Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.10/dist-packages (from papermill) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from papermill) (2.31.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from papermill) (0.4)\n","Requirement already satisfied: tenacity>=5.0.2 in /usr/local/lib/python3.10/dist-packages (from papermill) (8.2.3)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill) (6.1.12)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill) (5.5.0)\n","Requirement already satisfied: traitlets>=5.4 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.2.0->papermill) (5.7.1)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.2->papermill) (2.19.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1.2->papermill) (4.19.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->papermill) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->papermill) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->papermill) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->papermill) (2023.11.17)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (2023.11.2)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (0.31.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1.2->papermill) (0.13.2)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (23.2.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (2.8.2)\n","Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (6.3.2)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbclient>=0.2.0->papermill) (4.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->nbclient>=0.2.0->papermill) (1.16.0)\n","Collecting cornac\n","  Downloading cornac-1.18.0-cp310-cp310-manylinux1_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cornac) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from cornac) (1.11.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cornac) (4.66.1)\n","Collecting powerlaw (from cornac)\n","  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from powerlaw->cornac) (3.7.1)\n","Requirement already satisfied: mpmath in /usr/local/lib/python3.10/dist-packages (from powerlaw->cornac) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac) (4.46.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->powerlaw->cornac) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->powerlaw->cornac) (1.16.0)\n","Installing collected packages: powerlaw, cornac\n","Successfully installed cornac-1.18.0 powerlaw-1.5\n","Collecting retrying\n","  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying) (1.16.0)\n","Installing collected packages: retrying\n","Successfully installed retrying-1.3.4\n","Collecting pandera\n","  Downloading pandera-0.18.0-py3-none-any.whl (209 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multimethod (from pandera)\n","  Downloading multimethod-1.10-py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from pandera) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pandera) (23.2)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pandera) (1.5.3)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from pandera) (1.10.13)\n","Collecting typeguard>=3.0.2 (from pandera)\n","  Downloading typeguard-4.1.5-py3-none-any.whl (34 kB)\n","Collecting typing-inspect>=0.6.0 (from pandera)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from pandera) (1.14.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pandera) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pandera) (2023.3.post1)\n","Collecting typing-extensions>=4.7.0 (from typeguard>=3.0.2->pandera)\n","  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.6.0->pandera)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pandera) (1.16.0)\n","Installing collected packages: typing-extensions, mypy-extensions, multimethod, typing-inspect, typeguard, pandera\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed multimethod-1.10 mypy-extensions-1.0.0 pandera-0.18.0 typeguard-4.1.5 typing-extensions-4.9.0 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TNsnEKhZnPx1","executionInfo":{"status":"ok","timestamp":1702379159844,"user_tz":-420,"elapsed":14247,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}},"outputId":"6ea984c8-28c1-47e7-c48d-df93de1b4380"},"outputs":[{"output_type":"stream","name":"stdout","text":["System version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n","Pandas version: 1.5.3\n","Tensorflow version: 2.14.0\n"]}],"source":["import sys\n","import os\n","import shutil\n","import papermill as pm\n","import scrapbook as sb\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","tf.get_logger().setLevel('ERROR') # only show error messages\n","\n","from recommenders.utils.timer import Timer\n","from recommenders.models.ncf.ncf_singlenode import NCF\n","from recommenders.models.ncf.dataset import Dataset as NCFDataset\n","from recommenders.datasets import movielens\n","from recommenders.datasets.python_splitters import python_chrono_split\n","from recommenders.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k,\n","                                                     recall_at_k, get_top_k_items)\n","from recommenders.utils.constants import SEED as DEFAULT_SEED\n","\n","\n","print(\"System version: {}\".format(sys.version))\n","print(\"Pandas version: {}\".format(pd.__version__))\n","print(\"Tensorflow version: {}\".format(tf.__version__))"]},{"cell_type":"code","execution_count":6,"metadata":{"tags":["parameters"],"id":"RlHaiIElnPx3","executionInfo":{"status":"ok","timestamp":1702379164873,"user_tz":-420,"elapsed":543,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}}},"outputs":[],"source":["# top k items to recommend\n","TOP_K = 10\n","\n","# Select MovieLens data size: 100k, 1m, 10m, or 20m\n","MOVIELENS_DATA_SIZE = '100k'\n","\n","# Model parameters\n","EPOCHS = 100\n","BATCH_SIZE = 256\n","\n","SEED = DEFAULT_SEED  # Set None for non-deterministic results"]},{"cell_type":"markdown","metadata":{"id":"Mb0axb43nPx3"},"source":["## 1 Matrix factorization algorithm\n","\n","NCF is a neural matrix factorization model, which ensembles Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP) to unify the strengths of linearity of MF and non-linearity of MLP for modelling the user–item latent structures. NCF can be demonstrated as a framework for GMF and MLP, which is illustrated as below:"]},{"cell_type":"markdown","metadata":{"id":"AbCyiHXBnPx4"},"source":["<img src=\"https://recodatasets.z20.web.core.windows.net/images/NCF.svg?sanitize=true\">\n","\n","This figure shows how to utilize latent vectors of items and users, and then how to fuse outputs from GMF Layer (left) and MLP Layer (right). We will introduce this framework and show how to learn the model parameters in following sections.\n","\n","### 1.1 The GMF model\n","\n","In ALS, the ratings are modeled as follows:\n","\n","$$\\hat { r } _ { u , i } = q _ { i } ^ { T } p _ { u }$$\n","\n","GMF introduces a neural CF layer as the output layer of standard MF. In this way, MF can be easily generalized\n","and extended. For example, if we allow the edge weights of this output layer to be learnt from data without the uniform constraint, it will result in a variant of MF that allows varying importance of latent dimensions. And if we use a non-linear function for activation, it will generalize MF to a non-linear setting which might be more expressive than the linear MF model. GMF can be shown as follows:\n","\n","$$\\hat { r } _ { u , i } = a _ { o u t } \\left( h ^ { T } \\left( q _ { i } \\odot p _ { u } \\right) \\right)$$\n","\n","where $\\odot$ is element-wise product of vectors. Additionally, ${a}_{out}$ and ${h}$ denote the activation function and edge weights of the output layer respectively. MF can be interpreted as a special case of GMF. Intuitively, if we use an identity function for ${a}_{out}$ and enforce ${h}$ to be a uniform vector of 1, we can exactly recover the MF model.\n","\n","### 1.2 The MLP model\n","\n","NCF adopts two pathways to model users and items: 1) element-wise product of vectors, 2) concatenation of vectors. To learn interactions after concatenating of users and items latent features, the standard MLP model is applied. In this sense, we can endow the model a large level of flexibility and non-linearity to learn the interactions between $p_{u}$ and $q_{i}$. The details of MLP model are:\n","\n","For the input layer, there is concatenation of user and item vectors:\n","\n","$$z _ { 1 } = \\phi _ { 1 } \\left( p _ { u } , q _ { i } \\right) = \\left[ \\begin{array} { c } { p _ { u } } \\\\ { q _ { i } } \\end{array} \\right]$$\n","\n","So for the hidden layers and output layer of MLP, the details are:\n","\n","$$\n","\\phi _ { l } \\left( z _ { l } \\right) = a _ { o u t } \\left( W _ { l } ^ { T } z _ { l } + b _ { l } \\right) , ( l = 2,3 , \\ldots , L - 1 )\n","$$\n","\n","and:\n","\n","$$\n","\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\phi \\left( z _ { L - 1 } \\right) \\right)\n","$$\n","\n","where ${ W }_{ l }$, ${ b }_{ l }$, and ${ a }_{ out }$ denote the weight matrix, bias vector, and activation function for the $l$-th layer’s perceptron, respectively. For activation functions of MLP layers, one can freely choose sigmoid, hyperbolic tangent (tanh), and Rectifier (ReLU), among others. Because we have a binary classification task, the activation function of the output layer is defined as sigmoid $\\sigma(x)=\\frac{1}{1+e^{-x}}$ to restrict the predicted score to be in (0,1).\n","\n","\n","### 1.3 Fusion of GMF and MLP\n","\n","To provide more flexibility to the fused model, we allow GMF and MLP to learn separate embeddings, and combine the two models by concatenating their last hidden layer. We get $\\phi^{GMF}$ from GMF:\n","\n","$$\\phi _ { u , i } ^ { G M F } = p _ { u } ^ { G M F } \\odot q _ { i } ^ { G M F }$$\n","\n","and obtain $\\phi^{MLP}$ from MLP:\n","\n","$$\\phi _ { u , i } ^ { M L P } = a _ { o u t } \\left( W _ { L } ^ { T } \\left( a _ { o u t } \\left( \\ldots a _ { o u t } \\left( W _ { 2 } ^ { T } \\left[ \\begin{array} { c } { p _ { u } ^ { M L P } } \\\\ { q _ { i } ^ { M L P } } \\end{array} \\right] + b _ { 2 } \\right) \\ldots \\right) \\right) + b _ { L }\\right.$$\n","\n","Lastly, we fuse output from GMF and MLP:\n","\n","$$\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\left[ \\begin{array} { l } { \\phi ^ { G M F } } \\\\ { \\phi ^ { M L P } } \\end{array} \\right] \\right)$$\n","\n","This model combines the linearity of MF and non-linearity of DNNs for modelling user–item latent structures.\n","\n","### 1.4 Objective Function\n","\n","We define the likelihood function as:\n","\n","$$P \\left( \\mathcal { R } , \\mathcal { R } ^ { - } | \\mathbf { P } , \\mathbf { Q } , \\Theta \\right) = \\prod _ { ( u , i ) \\in \\mathcal { R } } \\hat { r } _ { u , i } \\prod _ { ( u , j ) \\in \\mathcal { R } ^{ - } } \\left( 1 - \\hat { r } _ { u , j } \\right)$$\n","\n","Where $\\mathcal{R}$ denotes the set of observed interactions, and $\\mathcal{ R } ^ { - }$ denotes the set of negative instances. $\\mathbf{P}$ and $\\mathbf{Q}$ denotes the latent factor matrix for users and items, respectively; and $\\Theta$ denotes the model parameters. Taking the negative logarithm of the likelihood, we obtain the objective function to minimize for NCF method, which is known as [binary cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy):\n","\n","$$L = - \\sum _ { ( u , i ) \\in \\mathcal { R } \\cup { \\mathcal { R } } ^ { - } } r _ { u , i } \\log \\hat { r } _ { u , i } + \\left( 1 - r _ { u , i } \\right) \\log \\left( 1 - \\hat { r } _ { u , i } \\right)$$\n","\n","The optimization can be done by performing Stochastic Gradient Descent (SGD), which is described in the [Surprise SVD deep dive notebook](surprise_svd_deep_dive.ipynb). Our SGD method is very similar to the SVD algorithm's."]},{"cell_type":"markdown","metadata":{"id":"ST3p_x_wnPx4"},"source":["## 2 TensorFlow implementation of NCF\n","\n","We will use the MovieLens dataset, which is composed of integer ratings from 1 to 5.\n","\n","We convert MovieLens into implicit feedback, and evaluate under our *leave-one-out* evaluation protocol.\n","\n","You can check the details of implementation in `recommenders/models/ncf`\n"]},{"cell_type":"markdown","metadata":{"id":"klsaMHdcnPx4"},"source":["## 3 TensorFlow NCF movie recommender\n","\n","### 3.1 Load and split data\n","\n","To evaluate the performance of item recommendation, we adopt the leave-one-out evaluation.\n","\n","For each user, we held out his/her last interaction as the test set and utilized the remaining data for training. Since it is too time-consuming to rank all items for every user during evaluation, we followed the common strategy that randomly samples 100 items that are not interacted by the user, ranking the test item among the 100 items. Our test samples will be constructed by `NCFDataset`.\n","\n","We also show an alternative evaluation method, splitting the data chronologically using `python_chrono_split` to achieve a 75/25% training and test split."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"MNuBFUp1nPx5","executionInfo":{"status":"ok","timestamp":1702379266839,"user_tz":-420,"elapsed":11,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}},"outputId":"adf27c2f-01c1-45bc-b74d-2ba0b4221b24"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   userID  itemID  rating  timestamp\n","0       1       1     4.0  964982703\n","1       1       3     4.0  964981247\n","2       1       6     4.0  964982224\n","3       1      47     5.0  964983815\n","4       1      50     5.0  964982931"],"text/html":["\n","  <div id=\"df-d04cea57-48cd-4a7b-afbe-adf890bbe826\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userID</th>\n","      <th>itemID</th>\n","      <th>rating</th>\n","      <th>timestamp</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4.0</td>\n","      <td>964982703</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4.0</td>\n","      <td>964981247</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>4.0</td>\n","      <td>964982224</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>47</td>\n","      <td>5.0</td>\n","      <td>964983815</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>50</td>\n","      <td>5.0</td>\n","      <td>964982931</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d04cea57-48cd-4a7b-afbe-adf890bbe826')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d04cea57-48cd-4a7b-afbe-adf890bbe826 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d04cea57-48cd-4a7b-afbe-adf890bbe826');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-928e24eb-526b-4ee6-9411-8e79347b23fb\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-928e24eb-526b-4ee6-9411-8e79347b23fb')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-928e24eb-526b-4ee6-9411-8e79347b23fb button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":10}],"source":["df = pd.read_csv('/content/drive/MyDrive/Colab_me/DS300/recommenders/recommenders/datasets/Movielens/ml-latest-small/ratings.csv')\n","df = df[[\"userId\", \"movieId\", \"rating\", \"timestamp\"]]\n","df = df.rename(columns={'userId': \"userID\", 'movieId': \"itemID\"})\n","\n","df.head()"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Qznlhh6knPx5","executionInfo":{"status":"ok","timestamp":1702379270313,"user_tz":-420,"elapsed":6,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}}},"outputs":[],"source":["train, test = python_chrono_split(df, 0.75)"]},{"cell_type":"markdown","metadata":{"id":"zgAN8wCxnPx5"},"source":["Filter out any users or items in the test set that do not appear in the training set."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"JulqOexOnPx5","executionInfo":{"status":"ok","timestamp":1702379274414,"user_tz":-420,"elapsed":6,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}}},"outputs":[],"source":["test = test[test[\"userID\"].isin(train[\"userID\"].unique())]\n","test = test[test[\"itemID\"].isin(train[\"itemID\"].unique())]"]},{"cell_type":"markdown","metadata":{"id":"hprExC9enPx6"},"source":["Create a test set containing the last interaction for each user as for the leave-one-out evaluation."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Dct69XRYnPx6","executionInfo":{"status":"ok","timestamp":1702379275945,"user_tz":-420,"elapsed":5,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}}},"outputs":[],"source":["leave_one_out_test = test.groupby(\"userID\").last().reset_index()"]},{"cell_type":"markdown","metadata":{"id":"HDx0nGR3nPx6"},"source":["Write datasets to csv files."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"_V1euAa4nPx6","executionInfo":{"status":"ok","timestamp":1702379280354,"user_tz":-420,"elapsed":726,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}}},"outputs":[],"source":["train_file = \"./train.csv\"\n","test_file = \"./test.csv\"\n","leave_one_out_test_file = \"./leave_one_out_test.csv\"\n","train.to_csv(train_file, index=False)\n","test.to_csv(test_file, index=False)\n","leave_one_out_test.to_csv(leave_one_out_test_file, index=False)"]},{"cell_type":"markdown","metadata":{"id":"jm-rISzNnPx6"},"source":["### 3.2 Functions of NCF Dataset\n","\n","Important functions of the Dataset class for NCF:\n","\n","`train_loader(batch_size, shuffle_size)`, generate training batches of size `batch_size`. Positive examples are loaded from the training file and negative samples are added in memory. `shuffle_size` determines the number of rows that are read into memory before the examples are shuffled. By default, the function will attempt to load all data before performing the shuffle. If memory constraints are encountered when using large datasets, try reducing `shuffle_size`.\n","\n","`test_loader()`, generate test batch by every positive test instance, (eg. `[1, 2, 1]` is a positive user & item pair in test set (`[userID, itemID, rating]` for this tuple). This function returns data like `[[1, 2, 1], [1, 3, 0], [1,6, 0], ...]`, ie. following our *leave-one-out* evaluation protocol."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWcvJ-KxnPx6","executionInfo":{"status":"ok","timestamp":1702379294803,"user_tz":-420,"elapsed":11690,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}},"outputId":"992b56ac-4790-4025-e49b-7720b2f0c24b"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 610/610 [00:10<00:00, 56.47it/s]\n"]}],"source":["data = NCFDataset(train_file=train_file, test_file=leave_one_out_test_file, seed=SEED, overwrite_test_file_full=True)"]},{"cell_type":"markdown","metadata":{"id":"6WaD9RtUnPx6"},"source":["### 3.3 Train NCF based on TensorFlow\n","The NCF has a lot of parameters. The most important ones are:\n","\n","`n_factors`, which controls the dimension of the latent space. Usually, the quality of the training set predictions grows with as n_factors gets higher.\n","\n","`layer_sizes`, sizes of input layer (and hidden layers) of MLP, input type is list.\n","\n","`n_epochs`, which defines the number of iteration of the SGD procedure.\n","Note that both parameter also affect the training time.\n","\n","`model_type`, we can train single `\"MLP\"`, `\"GMF\"` or combined model `\"NCF\"` by changing the type of model.\n","\n","We will here set `n_factors` to 4, `layer_sizes` to `[16,8,4]`,  `n_epochs` to 100, `batch_size` to 256. To train the model, we simply need to call the `fit()` method."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A1SNjIXVnPx6","executionInfo":{"status":"ok","timestamp":1702379298678,"user_tz":-420,"elapsed":1132,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}},"outputId":"995dc888-ef3a-444d-db2f-6bbbc1eb77aa"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1697: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  warnings.warn('`layer.apply` is deprecated and '\n"]}],"source":["model = NCF (\n","    n_users=data.n_users,\n","    n_items=data.n_items,\n","    model_type=\"NeuMF\",\n","    n_factors=4,\n","    layer_sizes=[16,8,4],\n","    n_epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    learning_rate=1e-3,\n","    verbose=10,\n","    seed=SEED\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"pU2QnIh4nPx6","executionInfo":{"status":"error","timestamp":1702380169209,"user_tz":-420,"elapsed":868343,"user":{"displayName":"Minh Nguyễn Hoàng","userId":"04529726266143512473"}},"outputId":"d9a6f4e0-dd1f-4343-e296-3d025f82acd2"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-cee165796ef0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Took {} seconds for training.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab_me/DS300/recommenders/recommenders/models/ncf/ncf_singlenode.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;31m# get loss and execute optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_begin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    973\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    974\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1216\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1217\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1396\u001b[0m                            run_metadata)\n\u001b[1;32m   1397\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1400\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1383\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1385\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1386\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1476\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1477\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1478\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1479\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m                                             run_metadata)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["with Timer() as train_time:\n","    model.fit(data)\n","\n","print(\"Took {} seconds for training.\".format(train_time.interval))"]},{"cell_type":"markdown","metadata":{"id":"HQHsGZnznPx7"},"source":["## 3.4 Prediction and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"pe98QejWnPx7"},"source":["### 3.4.1 Prediction\n","\n","Now that our model is fitted, we can call `predict` to get some `predictions`. `predict` returns an internal object Prediction which can be easily converted back to a dataframe:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHxmgxvKnPx7","outputId":"c3ce927a-039c-4c6b-84fe-3bd5c3990ef7"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>userID</th>\n","      <th>itemID</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>149.0</td>\n","      <td>0.029068</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>88.0</td>\n","      <td>0.624769</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>101.0</td>\n","      <td>0.234142</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>110.0</td>\n","      <td>0.101384</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>103.0</td>\n","      <td>0.067458</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   userID  itemID  prediction\n","0     1.0   149.0    0.029068\n","1     1.0    88.0    0.624769\n","2     1.0   101.0    0.234142\n","3     1.0   110.0    0.101384\n","4     1.0   103.0    0.067458"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["predictions = [[row.userID, row.itemID, model.predict(row.userID, row.itemID)]\n","               for (_, row) in test.iterrows()]\n","\n","\n","predictions = pd.DataFrame(predictions, columns=['userID', 'itemID', 'prediction'])\n","predictions.head()"]},{"cell_type":"markdown","metadata":{"id":"Ljss-9-QnPx7"},"source":["### 3.4.2 Generic Evaluation\n","We remove rated movies in the top k recommendations\n","To compute ranking metrics, we need predictions on all user, item pairs. We remove though the items already watched by the user, since we choose not to recommend them again."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33O0mki-nPx7","outputId":"c9491b68-a1b3-4315-c93d-c865f57898cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Took 2.7729760599977453 seconds for prediction.\n"]}],"source":["with Timer() as test_time:\n","\n","    users, items, preds = [], [], []\n","    item = list(train.itemID.unique())\n","    for user in train.userID.unique():\n","        user = [user] * len(item)\n","        users.extend(user)\n","        items.extend(item)\n","        preds.extend(list(model.predict(user, item, is_list=True)))\n","\n","    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n","\n","    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n","    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n","\n","print(\"Took {} seconds for prediction.\".format(test_time.interval))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95PPy90YnPx7","outputId":"d5113366-fc9d-425b-bb40-752b49556139"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAP:\t0.048144\n","NDCG:\t0.198384\n","Precision@K:\t0.176246\n","Recall@K:\t0.098700\n"]}],"source":["\n","eval_map = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n","eval_ndcg = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n","eval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n","eval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n","\n","print(\"MAP:\\t%f\" % eval_map,\n","      \"NDCG:\\t%f\" % eval_ndcg,\n","      \"Precision@K:\\t%f\" % eval_precision,\n","      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"]},{"cell_type":"markdown","metadata":{"id":"OtsaAelxnPx7"},"source":["### 3.4.3 \"Leave-one-out\" Evaluation\n","\n","We implement the functions to repoduce the leave-one-out evaluation protocol mentioned in original NCF paper.\n","\n","For each item in test data, we randomly samples 100 items that are not interacted by the user, ranking the test item among the 101 items (1 positive item and 100 negative items). The performance of a ranked list is judged by **Hit Ratio (HR)** and **Normalized Discounted Cumulative Gain (NDCG)**. Finally, we average the values of those ranked lists to obtain the overall HR and NDCG on test data.\n","\n","We truncated the ranked list at 10 for both metrics. As such, the HR intuitively measures whether the test item is present on the top-10 list, and the NDCG accounts for the position of the hit by assigning higher scores to hits at top ranks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WCjjGsB_nPx7","outputId":"07a6edde-2d50-4b21-835d-c820bff5ade5"},"outputs":[{"name":"stdout","output_type":"stream","text":["HR:\t0.506893\n","NDCG:\t0.401163\n"]}],"source":["k = TOP_K\n","\n","ndcgs = []\n","hit_ratio = []\n","\n","for b in data.test_loader():\n","    user_input, item_input, labels = b\n","    output = model.predict(user_input, item_input, is_list=True)\n","\n","    output = np.squeeze(output)\n","    rank = sum(output >= output[0])\n","    if rank <= k:\n","        ndcgs.append(1 / np.log(rank + 1))\n","        hit_ratio.append(1)\n","    else:\n","        ndcgs.append(0)\n","        hit_ratio.append(0)\n","\n","eval_ndcg = np.mean(ndcgs)\n","eval_hr = np.mean(hit_ratio)\n","\n","print(\"HR:\\t%f\" % eval_hr)\n","print(\"NDCG:\\t%f\" % eval_ndcg)\n"]},{"cell_type":"markdown","metadata":{"id":"j5ZP1tT-nPx8"},"source":["## 3.5 Pre-training\n","\n","To get better performance of NeuMF, we can adopt pre-training strategy. We first train GMF and MLP with random initializations until convergence. Then use their model parameters as the initialization for the corresponding parts of NeuMF’s parameters.  Please pay attention to the output layer, where we concatenate weights of the two models with\n","\n","$$h ^ { N C F } \\leftarrow \\left[ \\begin{array} { c } { \\alpha h ^ { G M F } } \\\\ { ( 1 - \\alpha ) h ^ { M L P } } \\end{array} \\right]$$\n","\n","where $h^{GMF}$ and $h^{MLP}$ denote the $h$ vector of the pretrained GMF and MLP model, respectively; and $\\alpha$ is a\n","hyper-parameter determining the trade-off between the two pre-trained models. We set $\\alpha$ = 0.5."]},{"cell_type":"markdown","metadata":{"id":"FxPY3SJ4nPx8"},"source":["### 3.5.1 Training GMF and MLP model\n","`model.save`, we can set the `dir_name` to store the parameters of GMF and MLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KryzRObSnPx8"},"outputs":[],"source":["model = NCF (\n","    n_users=data.n_users,\n","    n_items=data.n_items,\n","    model_type=\"GMF\",\n","    n_factors=4,\n","    layer_sizes=[16,8,4],\n","    n_epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    learning_rate=1e-3,\n","    verbose=10,\n","    seed=SEED\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcTWAKpTnPx8","outputId":"8f8b6a33-2be4-42e6-b280-3354fe6766bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Took 478.8678633829986 seconds for training.\n"]}],"source":["with Timer() as train_time:\n","    model.fit(data)\n","\n","print(\"Took {} seconds for training.\".format(train_time.interval))\n","\n","model.save(dir_name=\".pretrain/GMF\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CNNtIYRsnPx8"},"outputs":[],"source":["model = NCF (\n","    n_users=data.n_users,\n","    n_items=data.n_items,\n","    model_type=\"MLP\",\n","    n_factors=4,\n","    layer_sizes=[16,8,4],\n","    n_epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    learning_rate=1e-3,\n","    verbose=10,\n","    seed=SEED\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpeRv61TnPx_","outputId":"df8b1d54-b9cd-4caf-fee3-64abd3c82e01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Took 507.5963159920029 seconds for training.\n"]}],"source":["with Timer() as train_time:\n","    model.fit(data)\n","\n","print(\"Took {} seconds for training.\".format(train_time.interval))\n","\n","model.save(dir_name=\".pretrain/MLP\")"]},{"cell_type":"markdown","metadata":{"id":"GnetGcJwnPx_"},"source":["### 3.5.2 Load pre-trained GMF and MLP model for NeuMF\n","`model.load`, we can set the `gmf_dir` and `mlp_dir` to store the parameters for NeuMF."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aq-NhncbnPx_"},"outputs":[],"source":["model = NCF (\n","    n_users=data.n_users,\n","    n_items=data.n_items,\n","    model_type=\"NeuMF\",\n","    n_factors=4,\n","    layer_sizes=[16,8,4],\n","    n_epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    learning_rate=1e-3,\n","    verbose=10,\n","    seed=SEED\n",")\n","\n","model.load(gmf_dir=\".pretrain/GMF\", mlp_dir=\".pretrain/MLP\", alpha=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSG41nFSnPx_","outputId":"256e8edb-1fa2-4cb0-f2e9-7fe0d334363b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Took 616.8741841240007 seconds for training.\n"]}],"source":["with Timer() as train_time:\n","    model.fit(data)\n","\n","print(\"Took {} seconds for training.\".format(train_time.interval))"]},{"cell_type":"markdown","metadata":{"id":"XP37mUDrnPyL"},"source":["### 3.5.3 Compare with not pre-trained NeuMF\n","\n","You can use beforementioned evaluation methods to evaluate the pre-trained `NCF` Model. Usually, we will find the performance of pre-trained NCF is better than the not pre-trained."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNykYXennPyL","outputId":"cffb0db8-0ed6-4c7f-8888-c6b17f80b121"},"outputs":[{"name":"stdout","output_type":"stream","text":["Took 2.703430027999275 seconds for prediction.\n"]}],"source":["with Timer() as test_time:\n","\n","    users, items, preds = [], [], []\n","    item = list(train.itemID.unique())\n","    for user in train.userID.unique():\n","        user = [user] * len(item)\n","        users.extend(user)\n","        items.extend(item)\n","        preds.extend(list(model.predict(user, item, is_list=True)))\n","\n","    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n","\n","    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n","    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n","\n","print(\"Took {} seconds for prediction.\".format(test_time.interval))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnNmpEi9nPyM","outputId":"9f1639ee-cce4-4027-cc73-45790c4fdd82"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAP:\t0.044724\n","NDCG:\t0.183073\n","Precision@K:\t0.167020\n","Recall@K:\t0.096622\n"]}],"source":["eval_map2 = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n","eval_ndcg2 = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n","eval_precision2 = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n","eval_recall2 = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n","\n","print(\"MAP:\\t%f\" % eval_map2,\n","      \"NDCG:\\t%f\" % eval_ndcg2,\n","      \"Precision@K:\\t%f\" % eval_precision2,\n","      \"Recall@K:\\t%f\" % eval_recall2, sep='\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgYqpsTVnPyM","outputId":"7eee33c4-090f-426d-f14b-055d70c20f12"},"outputs":[{"data":{"application/scrapbook.scrap.json+json":{"data":0.04814371487113467,"encoder":"json","name":"map","version":1}},"metadata":{"scrapbook":{"data":true,"display":false,"name":"map"}},"output_type":"display_data"},{"data":{"application/scrapbook.scrap.json+json":{"data":0.4011627725677457,"encoder":"json","name":"ndcg","version":1}},"metadata":{"scrapbook":{"data":true,"display":false,"name":"ndcg"}},"output_type":"display_data"},{"data":{"application/scrapbook.scrap.json+json":{"data":0.1762460233297985,"encoder":"json","name":"precision","version":1}},"metadata":{"scrapbook":{"data":true,"display":false,"name":"precision"}},"output_type":"display_data"},{"data":{"application/scrapbook.scrap.json+json":{"data":0.09870014189901548,"encoder":"json","name":"recall","version":1}},"metadata":{"scrapbook":{"data":true,"display":false,"name":"recall"}},"output_type":"display_data"},{"data":{"application/scrapbook.scrap.json+json":{"data":0.04472389360593036,"encoder":"json","name":"map2","version":1}},"metadata":{"scrapbook":{"data":true,"display":false,"name":"map2"}},"output_type":"display_data"},{"data":{"application/scrapbook.scrap.json+json":{"data":0.1830730606672367,"encoder":"json","name":"ndcg2","version":1}},"metadata":{"scrapbook":{"data":true,"display":false,"name":"ndcg2"}},"output_type":"display_data"},{"data":{"application/scrapbook.scrap.json+json":{"data":0.1670201484623542,"encoder":"json","name":"precision2","version":1}},"metadata":{"scrapbook":{"data":true,"display":false,"name":"precision2"}},"output_type":"display_data"},{"data":{"application/scrapbook.scrap.json+json":{"data":0.09662158425458077,"encoder":"json","name":"recall2","version":1}},"metadata":{"scrapbook":{"data":true,"display":false,"name":"recall2"}},"output_type":"display_data"}],"source":["# Record results with papermill for tests\n","sb.glue(\"map\", eval_map)\n","sb.glue(\"ndcg\", eval_ndcg)\n","sb.glue(\"precision\", eval_precision)\n","sb.glue(\"recall\", eval_recall)\n","sb.glue(\"map2\", eval_map2)\n","sb.glue(\"ndcg2\", eval_ndcg2)\n","sb.glue(\"precision2\", eval_precision2)\n","sb.glue(\"recall2\", eval_recall2)"]},{"cell_type":"markdown","metadata":{"id":"DGiVTY25nPyM"},"source":["### 3.5.4 Delete pre-trained directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dtT38SGnPyM","outputId":"7c0d1ae6-5260-4402-e43f-be0f49b2e769"},"outputs":[{"name":"stdout","output_type":"stream","text":["Did '.pretrain' exist?: False\n"]}],"source":["save_dir = \".pretrain\"\n","if os.path.exists(save_dir):\n","    shutil.rmtree(save_dir)\n","\n","print(\"Did \\'%s\\' exist?: %s\" % (save_dir, os.path.exists(save_dir)))"]},{"cell_type":"markdown","metadata":{"id":"kCD8XP62nPyM"},"source":["### Reference:\n","1. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu & Tat-Seng Chua, Neural Collaborative Filtering, 2017, https://arxiv.org/abs/1708.05031\n","\n","2. Official NCF implementation [Keras with Theano]: https://github.com/hexiangnan/neural_collaborative_filtering\n","\n","3. Other nice NCF implementation [Pytorch]: https://github.com/LaceyChen17/neural-collaborative-filtering"]}],"metadata":{"celltoolbar":"Tags","interpreter":{"hash":"3a9a0c422ff9f08d62211b9648017c63b0a26d2c935edc37ebb8453675d13bb5"},"kernelspec":{"display_name":"reco_gpu","language":"python","name":"conda-env-reco_gpu-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}